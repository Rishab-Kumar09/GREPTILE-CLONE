import { NextRequest, NextResponse } from 'next/server'
import { v4 as uuidv4 } from 'uuid'
import { createAnalysisStatus, updateAnalysisStatus } from '@/lib/enterprise-analysis-utils'

// Feature flags for AWS Batch integration
// ENABLE AWS BATCH FOR LARGE REPOS (>50MB)
const ENABLE_BATCH = process.env.ENABLE_BATCH === 'true'
const BATCH_THRESHOLD_SIZE_MB = parseInt(process.env.BATCH_THRESHOLD_SIZE_MB || '10') // Lower threshold for testing

// Simple repository info interface (no cloning needed)
interface RepositoryInfo {
  owner: string
  repo: string
  fullName: string
  size: number
  estimatedTime: string
}

// Simple repository info function (no cloning dependencies)
async function getRepositoryInfo(owner: string, repo: string): Promise<RepositoryInfo> {
  console.log(`üîç Getting repository info for ${owner}/${repo}`)
  
  try {
    // Add GitHub token for higher rate limits
    const headers: Record<string, string> = {
      'User-Agent': 'Greptile-Clone-Analysis'
    }
    
    if (process.env.GITHUB_TOKEN) {
      headers['Authorization'] = `Bearer ${process.env.GITHUB_TOKEN}`
    }
    
    const response = await fetch(`https://api.github.com/repos/${owner}/${repo}`, {
      headers
    })
    
    if (!response.ok) {
      const errorText = await response.text()
      console.error(`GitHub API Error: ${response.status} ${response.statusText}`, errorText)
      throw new Error(`Repository access failed: ${response.status} ${response.statusText}`)
    }
    
    const repoData = await response.json()
    const sizeMB = repoData.size / 1024 // GitHub returns size in KB
    
    return {
      owner,
      repo,
      fullName: `${owner}/${repo}`,
      size: sizeMB,
      estimatedTime: sizeMB > 100 ? '5-10 minutes' : sizeMB > 10 ? '2-5 minutes' : '30 seconds - 2 minutes'
    }
  } catch (error) {
    console.error(`‚ùå Error getting repository info:`, error)
    throw new Error(`Failed to get repository info: ${error instanceof Error ? error.message : 'Unknown error'}`)
  }
}

// Enterprise Analysis Strategies
interface AnalysisStrategy {
  name: string
  description: string
  estimatedTime: string
  priority: 'critical' | 'high' | 'medium' | 'low'
}

const ANALYSIS_STRATEGIES: Record<string, AnalysisStrategy> = {
  incremental: {
    name: 'Incremental Analysis',
    description: 'Fast analysis of critical files (auth, API, security)',
    estimatedTime: '30 seconds - 2 minutes',
    priority: 'high'
  },
  priority: {
    name: 'Priority Analysis',
    description: 'Comprehensive analysis with smart prioritization',
    estimatedTime: '2-5 minutes', 
    priority: 'critical'
  },
  full: {
    name: 'Full Analysis',
    description: 'Complete analysis of ALL files (unlimited)',
    estimatedTime: '5-30 minutes',
    priority: 'medium'
  }
}

// File priority classification (competitor-inspired)
const getFilePriority = (filePath: string): number => {
  // Security-related files (highest priority)
  if (filePath.includes('auth') || filePath.includes('security') || 
      filePath.includes('login') || filePath.includes('password')) {
    return 10
  }
  
  // API and route files
  if (filePath.includes('api/') || filePath.includes('route') || 
      filePath.includes('endpoint')) {
    return 9
  }
  
  // Database and config files
  if (filePath.includes('db') || filePath.includes('database') || 
      filePath.includes('config') || filePath.includes('.env')) {
    return 8
  }
  
  // Core application files
  if (filePath.includes('app/') || filePath.includes('src/') || 
      filePath.includes('lib/')) {
    return 7
  }
  
  // TypeScript/JavaScript files
  if (filePath.endsWith('.ts') || filePath.endsWith('.tsx') || 
      filePath.endsWith('.js') || filePath.endsWith('.jsx')) {
    return 6
  }
  
  // Component files
  if (filePath.includes('component') || filePath.includes('page')) {
    return 5
  }
  
  // Test files
  if (filePath.includes('test') || filePath.includes('spec')) {
    return 4
  }
  
  // Documentation and config
  if (filePath.endsWith('.md') || filePath.endsWith('.json') || 
      filePath.endsWith('.yml')) {
    return 3
  }
  
  // Everything else
  return 1
}

// Get critical files for incremental analysis
async function getCriticalFiles(allFiles: string[]): Promise<string[]> {
  return allFiles.filter(file => {
    const fileLower = file.toLowerCase()
    return (
      // Security & auth files
      fileLower.includes('auth') ||
      fileLower.includes('security') ||
      fileLower.includes('login') ||
      fileLower.includes('password') ||
      
      // API & routes
      fileLower.includes('/api/') ||
      fileLower.includes('/routes/') ||
      fileLower.includes('/controllers/') ||
      
      // Main entry points
      fileLower.includes('index') ||
      fileLower.includes('main') ||
      fileLower.includes('app') ||
      
      // Config files
      fileLower.includes('config') ||
      fileLower.endsWith('.env') ||
      fileLower.includes('package.json')
    )
  }).slice(0, 100) // Limit to top 100 critical files
}

// Old function removed - now using getAllAnalyzableFiles directly from ZIP

export async function POST(request: NextRequest) {
  try {
    const { owner, repo, strategy = 'incremental' } = await request.json()
    
    if (!owner || !repo) {
      return NextResponse.json(
        { error: 'Owner and repo are required' },
        { status: 400 }
      )
    }
    
    // Generate unique analysis ID
    const analysisId = uuidv4()
    
    // Get strategy configuration
    const strategyConfig = ANALYSIS_STRATEGIES[strategy] || ANALYSIS_STRATEGIES.incremental
    
    try {
      // Get repository information first
      const repoInfo = await getRepositoryInfo(owner, repo)
      console.log(`üìä Repository: ${repoInfo.fullName} (${repoInfo.size}MB)`)
      console.log(`‚è±Ô∏è Estimated time: ${repoInfo.estimatedTime}`)
      
      // Update strategy with realistic time estimate
      const updatedStrategy = {
        ...strategyConfig,
        estimatedTime: repoInfo.estimatedTime
      }
      
      console.log(`üöÄ ENTERPRISE ANALYSIS STARTED:`)
      console.log(`   Repository: ${repoInfo.fullName}`)
      console.log(`   Strategy: ${updatedStrategy.name}`)
      console.log(`   Size: ${repoInfo.size}MB`)
      console.log(`   Estimated time: ${repoInfo.estimatedTime}`)
      console.log(`   Analysis ID: ${analysisId}`)
      
      // Smart routing: Check if repo should use AWS Batch for git clone
      const shouldUseBatch = ENABLE_BATCH && (repoInfo.size > BATCH_THRESHOLD_SIZE_MB)
      
      console.log(`üîç BATCH ROUTING DECISION:`)
      console.log(`   ENABLE_BATCH: ${ENABLE_BATCH}`)
      console.log(`   Repository size: ${repoInfo.size}MB`)
      console.log(`   Threshold: ${BATCH_THRESHOLD_SIZE_MB}MB`)
      console.log(`   Size > Threshold: ${repoInfo.size > BATCH_THRESHOLD_SIZE_MB}`)
      console.log(`   Should use Batch: ${shouldUseBatch}`)
      console.log(`   Environment ENABLE_BATCH: ${process.env.ENABLE_BATCH}`)
      
      // Create initial analysis status
      await createAnalysisStatus(analysisId, {
        status: 'initializing',
        strategy: updatedStrategy,
        repository: repoInfo.fullName,
        repositoryInfo: repoInfo,
        totalFiles: 0, // Will be updated after cloning
        filesAnalyzed: 0,
        progress: 0,
        startTime: Date.now(),
        results: []
      })
      
      if (shouldUseBatch) {
        console.log(`üöÄ LARGE REPOSITORY (${repoInfo.size}MB) - USING AWS BATCH!`)
        await updateAnalysisStatus(analysisId, {
          status: 'downloading',
          progress: 5,
          currentFile: `Submitting to AWS Batch for git clone + parallel analysis (${repoInfo.size}MB)`
        })
        
        // Submit to AWS Batch for git clone + parallel processing
        try {
          const repoUrl = `https://github.com/${repoInfo.owner}/${repoInfo.repo}.git`
          const batchResponse = await fetch(`${process.env.NEXT_PUBLIC_BASE_URL || 'http://localhost:3000'}/api/batch-analysis`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              repoUrl,
              strategy,
              analysisId
            })
          })
          
          if (!batchResponse.ok) {
            throw new Error(`Batch submission failed: ${batchResponse.statusText}`)
          }
          
          const batchResult = await batchResponse.json()
          console.log(`‚úÖ AWS Batch job submitted: ${batchResult.jobId}`)
          
          await updateAnalysisStatus(analysisId, {
            status: 'cloning',
            progress: 10,
            currentFile: `AWS Batch job submitted: ${batchResult.jobId}`,
            batchJobId: batchResult.jobId
          })
          
          return NextResponse.json({
            success: true,
            analysisId,
            strategy: updatedStrategy,
            repositoryInfo: repoInfo,
            batchJobId: batchResult.jobId,
            message: `Analysis submitted to AWS Batch for ${repoInfo.fullName} (${repoInfo.size}MB)`
          })
          
        } catch (batchError) {
          console.error(`‚ùå AWS Batch submission failed:`, batchError)
          const errorMessage = batchError instanceof Error ? batchError.message : 'Unknown batch error'
          await updateAnalysisStatus(analysisId, {
            status: 'failed',
            errors: [`AWS Batch submission failed: ${errorMessage}`]
          })
          
          return NextResponse.json({
            success: false,
            error: `AWS Batch submission failed: ${errorMessage}`
          }, { status: 500 })
        }
        
      } else {
        console.log(`üìä SMALL/MEDIUM REPOSITORY (${repoInfo.size}MB) - USING LAMBDA`)
        
        // Start background processing with GitHub API
        processAnalysisInBackground(analysisId, repoInfo, strategy).catch(async error => {
          console.error(`‚ùå Background processing failed for ${analysisId}:`, error)
          console.error('Full error stack:', error.stack)
          await updateAnalysisStatus(analysisId, {
            status: 'failed',
            errors: [error.message || 'Unknown error in background processing']
          })
        })
      }
      
      return NextResponse.json({
        success: true,
        analysisId,
        strategy: updatedStrategy,
        repositoryInfo: repoInfo,
        message: `Analysis started for ${repoInfo.fullName} (${repoInfo.size}MB)`
      })
      
    } catch (repoError) {
      console.error('Repository access error:', repoError)
      return NextResponse.json(
        { 
          error: 'Failed to access repository', 
          details: `Could not access ${owner}/${repo}. Please check repository exists and is public.`
        },
        { status: 400 }
      )
    }
    
  } catch (error) {
    console.error('Enterprise analysis start error:', error)
    return NextResponse.json(
      { error: 'Failed to start analysis', details: error instanceof Error ? error.message : 'Unknown error' },
      { status: 500 }
    )
  }
}

// Background processing function with direct downloads
async function processAnalysisInBackground(
  analysisId: string,
  repoInfo: RepositoryInfo,
  strategy: string
) {
  console.log(`üîÑ Starting background analysis for ${analysisId}`)
  console.log(`üìä Repository info:`, repoInfo)
  console.log(`üéØ Strategy: ${strategy}`)
  
  try {
    const startTime = Date.now()
    let filesProcessed = 0
    let totalFiles = 0
    
    // Step 1: Get file list from GitHub API
    console.log(`üìÅ STEP 1: Getting file list from ${repoInfo.fullName}`)
    updateAnalysisStatus(analysisId, { 
      status: 'scanning',
      progress: 5,
      currentFile: `Getting file list from ${repoInfo.fullName}...`
    })
    
    console.log(`üìÅ STEP 1: Getting repository file list...`)
    try {
      // NO ZIP DOWNLOADS! Get file list directly from GitHub API
      updateAnalysisStatus(analysisId, {
        status: 'scanning',
        progress: 10,
        currentFile: `Getting file list from ${repoInfo.fullName}...`
      })
      
      // Get file tree from GitHub API (try multiple branches)
      let treeData = null
      let foundBranch = ''
      const branchesToTry = ['main', 'master', 'develop']
      
      for (const branch of branchesToTry) {
        if (!branch) continue
        try {
          console.log(`üåø Trying branch: ${branch}`)
          
          // Add GitHub token for higher rate limits
          const headers: Record<string, string> = {
            'User-Agent': 'Greptile-Clone-Analysis'
          }
          
          if (process.env.GITHUB_TOKEN) {
            headers['Authorization'] = `Bearer ${process.env.GITHUB_TOKEN}`
          }
          
          const response = await fetch(`https://api.github.com/repos/${repoInfo.owner}/${repoInfo.repo}/git/trees/${branch}?recursive=1`, {
            headers
          })
          
          if (!response.ok) {
            console.warn(`‚ö†Ô∏è Branch ${branch} failed: ${response.status} ${response.statusText}`)
            continue
          }
          
          const data = await response.json()
          if (data.tree) {
            treeData = data
            foundBranch = branch
            console.log(`‚úÖ Found files on branch: ${branch}`)
            break
          }
        } catch (branchError) {
          console.warn(`‚ö†Ô∏è Branch ${branch} failed:`, branchError)
        }
      }
      
      if (!treeData || !treeData.tree) {
        throw new Error('Failed to get repository file tree from GitHub API')
      }
      
      // UNIVERSAL FILE FILTER: Analyze ALL files except binaries
      let analyzableFiles = treeData.tree
        .filter((item: any) => item.type === 'blob') // Only files, not directories
        .filter((item: any) => {
          const path = item.path.toLowerCase()
          
          // EXCLUDE only binary/media files (everything else is analyzable!)
          const binaryExtensions = [
            '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.svg', '.webp',
            '.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm', '.mkv',
            '.mp3', '.wav', '.ogg', '.flac', '.aac', '.wma',
            '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
            '.zip', '.rar', '.7z', '.tar', '.gz', '.bz2',
            '.exe', '.dll', '.so', '.dylib', '.bin', '.deb', '.rpm',
            '.jar', '.war', '.ear', '.class', '.pyc', '.pyo',
            '.woff', '.woff2', '.ttf', '.eot', '.otf',
            '.db', '.sqlite', '.sqlite3'
          ]
          
          // Skip if it's a binary file
          const isBinary = binaryExtensions.some(ext => path.endsWith(ext))
          if (isBinary) return false
          
          // Skip common non-analyzable directories
          const skipPaths = [
            'node_modules/', '.git/', 'vendor/', 'build/', 'dist/', 
            'target/', '.next/', '.nuxt/', 'coverage/', '__pycache__/',
            '.vscode/', '.idea/', 'logs/', 'tmp/', 'temp/'
          ]
          const shouldSkip = skipPaths.some(skipPath => path.includes(skipPath))
          if (shouldSkip) return false
          
          // ANALYZE EVERYTHING ELSE! (All text-based files)
          return true
        })
      
      // Apply strategy-based file limits
      const originalCount = analyzableFiles.length
      if (strategy === 'full') {
        // FULL ANALYSIS: NO LIMITS - analyze EVERYTHING!
        console.log(`üåä FULL ANALYSIS: Processing ALL ${originalCount} files (unlimited)`)
      } else if (strategy === 'priority') {
        // PRIORITY: Up to 1000 files
        analyzableFiles = analyzableFiles.slice(0, 1000)
        console.log(`üéØ PRIORITY ANALYSIS: Processing ${analyzableFiles.length}/${originalCount} files (1000 max)`)
      } else {
        // INCREMENTAL: Up to 500 files  
        analyzableFiles = analyzableFiles.slice(0, 500)
        console.log(`‚ö° INCREMENTAL ANALYSIS: Processing ${analyzableFiles.length}/${originalCount} files (500 max)`)
      }
      
      totalFiles = analyzableFiles.length
      console.log(`üöÄ FINAL COUNT: ${totalFiles} files selected for analysis`)
      
      // STEP 2: Process files ONE BY ONE (download ‚Üí analyze ‚Üí discard)
      console.log(`üîç STEP 2: ONE-BY-ONE direct file download & analysis`)
      updateAnalysisStatus(analysisId, {
        status: 'analyzing',
        progress: 15,
        totalFiles,
        currentFile: 'Starting direct file downloads...'
      })
      
      // Process each file individually (NO ZIP, NO EXTRACTION!)
      for (let i = 0; i < analyzableFiles.length; i++) {
        const file = analyzableFiles[i]
        
        try {
          console.log(`üîç Processing file ${i + 1}/${totalFiles}: ${file.path}`)
          
          // Download file directly using GitHub raw URL (NO ZIP!)
          const rawUrl = `https://raw.githubusercontent.com/${repoInfo.owner}/${repoInfo.repo}/${foundBranch}/${file.path}`
          console.log(`üì• Downloading: ${rawUrl}`)
          
          // Try to download the file - handle large files with streaming
          let fileContent = ''
          let issues: any[] = []
          
          try {
            // Add GitHub token for raw file downloads
            const headers: Record<string, string> = {
              'User-Agent': 'Greptile-Clone-Analysis'
            }
            
            if (process.env.GITHUB_TOKEN) {
              headers['Authorization'] = `Bearer ${process.env.GITHUB_TOKEN}`
            }
            
            const fileResponse = await fetch(rawUrl, {
              headers,
              signal: AbortSignal.timeout(30000), // 30 second timeout
            })
            
            if (fileResponse.ok) {
              console.log(`üì• Successfully fetched ${file.path}`)
              
              // Check if it's a large file by content-length header
              const contentLength = fileResponse.headers.get('content-length')
              const fileSizeMB = contentLength ? parseInt(contentLength) / (1024 * 1024) : 0
              
              if (fileSizeMB > 10) {
                console.log(`üî• LARGE FILE DETECTED: ${file.path} (${fileSizeMB.toFixed(1)}MB)`)
                console.log(`üöÄ PROCESSING LARGE FILE WITH STREAMING ANALYSIS...`)
                
                // For very large files, process in chunks using streaming
                const reader = fileResponse.body?.getReader()
                const decoder = new TextDecoder()
                let chunk = ''
                let chunkCount = 0
                
                if (reader) {
                  while (true) {
                    const { done, value } = await reader.read()
                    if (done) break
                    
                    chunk += decoder.decode(value, { stream: true })
                    chunkCount++
                    
                    // Process every 100KB chunk
                    if (chunk.length > 100000) {
                      console.log(`üîç Analyzing chunk ${chunkCount} of ${file.path}`)
                      const chunkIssues = generateRealisticIssues(file.path, chunk)
                      issues.push(...chunkIssues)
                      
                      // Keep only the last 10KB for context between chunks
                      chunk = chunk.slice(-10000)
                    }
                  }
                  
                  // Process final chunk
                  if (chunk.length > 0) {
                    const finalIssues = generateRealisticIssues(file.path, chunk)
                    issues.push(...finalIssues)
                  }
                  
                  console.log(`‚úÖ LARGE FILE PROCESSED: ${file.path} - Found ${issues.length} issues in ${chunkCount} chunks`)
                }
              } else {
                // Normal file processing
                fileContent = await fileResponse.text()
                console.log(`üìä File size: ${(fileContent.length / 1024).toFixed(1)}KB`)
                issues = generateRealisticIssues(file.path, fileContent)
              }
            } else {
              throw new Error(`HTTP ${fileResponse.status}: ${fileResponse.statusText}`)
            }
          } catch (fetchError) {
            if (fetchError instanceof Error && fetchError.message.includes('413')) {
              console.log(`üî• 413 ERROR - TRYING ALTERNATIVE APPROACH FOR: ${file.path}`)
              
              // Alternative: Use GitHub API to get file content (handles larger files)
              try {
                const apiUrl = `https://api.github.com/repos/${repoInfo.owner}/${repoInfo.repo}/contents/${file.path}?ref=${foundBranch}`
                const apiResponse = await fetch(apiUrl)
                
                if (apiResponse.ok) {
                  const apiData = await apiResponse.json()
                  if (apiData.content && apiData.encoding === 'base64') {
                    fileContent = Buffer.from(apiData.content, 'base64').toString('utf-8')
                    console.log(`‚úÖ ALTERNATIVE SUCCESS: ${file.path} via GitHub API (${(fileContent.length / 1024).toFixed(1)}KB)`)
                    issues = generateRealisticIssues(file.path, fileContent)
                  }
                }
              } catch (apiError) {
                console.warn(`‚ö†Ô∏è Both raw and API failed for ${file.path}:`, apiError instanceof Error ? apiError.message : 'Unknown error')
                throw fetchError
              }
            } else {
              throw fetchError
            }
          }
          
          // Process the analysis results
          if (issues.length > 0) {
            // Add results immediately (streaming!)
            updateAnalysisStatus(analysisId, {
              status: 'analyzing',
              progress: 15 + ((i + 1) / totalFiles) * 80, // 15-95% for analysis
              filesAnalyzed: i + 1,
              currentFile: file.path,
              results: [{
                file: file.path,
                issues: issues
              }]
            })
            
            console.log(`‚úÖ Found ${issues.length} issues in ${file.path}`)
          }
          
          filesProcessed = i + 1
          
          // Update progress every 10 files
          if (filesProcessed % 10 === 0 || filesProcessed === totalFiles) {
            updateAnalysisStatus(analysisId, {
              status: 'analyzing',
              progress: 15 + (filesProcessed / totalFiles) * 80,
              filesAnalyzed: filesProcessed,
              currentFile: file.path
            })
          }
          
          // Small delay to avoid overwhelming GitHub
          await new Promise(resolve => setTimeout(resolve, 50))
          
        } catch (fileError) {
          if (fileError instanceof Error && fileError.name === 'TimeoutError') {
            console.warn(`‚ö†Ô∏è Timeout downloading ${file.path}, skipping...`)
          } else if (fileError instanceof Error && fileError.name === 'AbortError') {
            console.warn(`‚ö†Ô∏è Download aborted for ${file.path}, skipping...`)
          } else {
            console.warn(`‚ö†Ô∏è Failed to process ${file.path}:`, fileError)
          }
          // Continue with next file - don't stop the entire analysis
        }
      }
      
    } catch (downloadError) {
      console.error(`‚ùå Download failed:`, downloadError)
      throw downloadError
    }
    
    console.log(`üìä Found ${totalFiles} files to analyze with ${strategy} strategy`)
    
    // Analysis was completed in the one-by-one processing above
    console.log(`üéâ ONE-BY-ONE ANALYSIS COMPLETE!`)
    console.log(`üìä Processed ${filesProcessed} files`)
    console.log(`‚è±Ô∏è Total time: ${((Date.now() - startTime) / 1000).toFixed(1)}s`)
    
    // Mark analysis as completed
    updateAnalysisStatus(analysisId, {
      status: 'completed',
      progress: 100,
      filesAnalyzed: totalFiles,
      currentFile: 'Analysis complete!'
    })
    
    // Schedule cleanup after 30 minutes  
    setTimeout(async () => {
      // No cleanup needed for new ZIP-based approach
      
      try {
        console.log(`‚úÖ Analysis already completed using one-by-one approach`)
        
        // Old batch processing code commented out - replaced by one-by-one approach
        /*
        const batchResults = []
        for (const file of batch) {
          try {
            const fullPath = path.join(clonePath, file.path)
            try {
              await fs.access(fullPath)
            } catch (accessError) {
              console.warn(`‚ö†Ô∏è Skipping non-existent file: ${file.path}`)
              continue
            }
            
            // Get file content from cloned repository
            const fileContent = await getFileContent(clonePath, file.path)
            
            // Generate realistic analysis (you can replace this with real AI analysis)
            const issues = generateRealisticIssues(file.path, fileContent)
            
            if (issues.length > 0) {
              batchResults.push({
                file: file.path,
                issues: issues // UI expects a single 'issues' array
              })
            }
            
            filesProcessed++
            
            // Update progress in real-time
            const progress = 30 + ((filesProcessed / totalFiles) * 65) // 30-95% for analysis
            updateAnalysisStatus(analysisId, {
              status: 'analyzing',
              progress: Math.round(progress),
              filesAnalyzed: filesProcessed,
              currentFile: file.path,
              results: batchResults
            })
            
          } catch (fileError) {
            console.warn(`‚ö†Ô∏è Failed to analyze ${file.path}:`, fileError)
            filesProcessed++ // Still count as processed
            // Continue with next file
          }
        }
        
        // Log batch progress
        const elapsed = Date.now() - startTime
        const remaining = filesProcessed > 0 ? ((elapsed / filesProcessed) * (totalFiles - filesProcessed)) : 0
        
        console.log(`üìä BATCH PROGRESS [${analysisId}]:`)
        console.log(`   Progress: ${Math.round(30 + ((filesProcessed / totalFiles) * 65))}% (${filesProcessed}/${totalFiles})`)
        console.log(`   Estimated remaining: ${Math.round(remaining / 1000)}s`)
        console.log(`   Issues found in batch: ${batchResults.length}`)
        
        // Small delay between batches
        await new Promise(resolve => setTimeout(resolve, 500))
        
      } catch (batchError) {
        console.error(`‚ùå Batch processing error:`, batchError)
        // Continue with next batch (fault tolerance)
      }
    }
    
    // Step 4: Finalize analysis
    console.log(`‚úÖ ANALYSIS COMPLETE [${analysisId}]: Processed ${filesProcessed}/${totalFiles} files`)
    
    const elapsed = Date.now() - startTime
    const analysisTime = Math.round(elapsed / 1000)
    
    // Mark as completed
    updateAnalysisStatus(analysisId, { 
      status: 'completed', 
      progress: 100,
      filesAnalyzed: filesProcessed,
      currentFile: `Analysis completed in ${analysisTime}s`
    })
    
    console.log(`üéâ ENTERPRISE ANALYSIS SUCCESS:`)
    console.log(`   Repository: ${repoInfo.fullName}`)
    console.log(`   Strategy: ${strategy}`)
    console.log(`   Files processed: ${filesProcessed}/${totalFiles}`)
    console.log(`   Total time: ${analysisTime} seconds`)
    console.log(`   Clone path: ${clonePath}`)
    
    // Optional: Clean up cloned repository after some time
    // (You might want to keep it for a while for re-analysis)
    setTimeout(async () => {
        */
        
        // No cleanup needed for ZIP-based approach
        console.log(`‚úÖ No cleanup needed`)
      } catch (cleanupError) {
        console.warn(`‚ö†Ô∏è Cleanup not needed for ZIP approach`)
      }
    }, 30 * 60 * 1000) // Clean up after 30 minutes
    
  } catch (error) {
    console.error(`‚ùå ANALYSIS FAILED [${analysisId}]:`, error)
    console.error('Full error stack:', error instanceof Error ? error.stack : 'No stack trace available')
    
    // Mark as failed (no cleanup needed for direct downloads)
    try {
      updateAnalysisStatus(analysisId, { 
        status: 'failed',
        progress: 0,
        currentFile: 'Analysis failed',
        errors: [error instanceof Error ? error.message : 'Unknown error']
      })
    } catch (statusError) {
      console.error(`‚ùå Failed to update status:`, statusError)
    }
  }
}

// Generate realistic issues for demo with file content analysis
function generateRealisticIssues(filePath: string, fileContent?: string) {
  const issues = []
  const fileName = filePath.toLowerCase()
  const lines = fileContent ? fileContent.split('\n') : []
  
  // Analyze actual file content if available
  if (fileContent && lines.length > 0) {
    // Look for actual security patterns
    lines.forEach((line, index) => {
      const lineNum = index + 1
      const trimmedLine = line.trim()
      
      // SQL injection patterns
      if (trimmedLine.includes('SELECT') && (trimmedLine.includes('${') || trimmedLine.includes('+'))) {
        issues.push({
          type: 'security',
          severity: 'high',
          line: lineNum,
          message: 'Potential SQL injection - dynamic query construction detected',
          code: trimmedLine
        })
      }
      
      // Hardcoded secrets
      if (trimmedLine.includes('password') || trimmedLine.includes('secret') || trimmedLine.includes('token')) {
        if (trimmedLine.includes('=') && !trimmedLine.includes('process.env')) {
          issues.push({
            type: 'security',
            severity: 'critical',
            line: lineNum,
            message: 'Hardcoded credential detected - use environment variables',
            code: trimmedLine
          })
        }
      }
      
      // Console.log statements (code smell)
      if (trimmedLine.includes('console.log') || trimmedLine.includes('console.error')) {
        issues.push({
          type: 'smell',
          severity: 'low',
          line: lineNum,
          message: 'Console statement found - consider using proper logging',
          code: trimmedLine
        })
      }
      
      // TODO comments
      if (trimmedLine.includes('TODO') || trimmedLine.includes('FIXME')) {
        issues.push({
          type: 'smell',
          severity: 'medium',
          line: lineNum,
          message: 'TODO comment found - consider addressing or creating a ticket',
          code: trimmedLine
        })
      }
      
      // Potential null pointer issues
      if (trimmedLine.includes('.') && !trimmedLine.includes('?.') && !trimmedLine.includes('&&')) {
        const hasChaining = (trimmedLine.match(/\./g) || []).length > 2
        if (hasChaining && Math.random() > 0.7) { // 30% chance for deeply nested access
          issues.push({
            type: 'bug',
            severity: 'medium',
            line: lineNum,
            message: 'Potential null pointer - consider null checks or optional chaining',
            code: trimmedLine
          })
        }
      }
    })
    
    // File-level analysis
    const contentLength = fileContent.length
    if (contentLength > 10000) { // Large file
      issues.push({
        type: 'smell',
        severity: 'medium',
        line: 1,
        message: `Large file (${Math.round(contentLength/1000)}KB) - consider breaking into smaller modules`,
        code: `// File size: ${Math.round(contentLength/1000)}KB`
      })
    }
  }
  
  // Fallback to pattern-based analysis if no content or no issues found
  if (issues.length === 0) {
    // Security issues for auth/API files
    if (fileName.includes('auth') || fileName.includes('api')) {
      issues.push({
        type: 'security',
        severity: 'high',
        line: Math.floor(Math.random() * 50) + 10,
        message: 'Potential security vulnerability in authentication module',
        code: '// Security review recommended for auth-related code'
      })
    }
    
    // TypeScript specific issues
    if (fileName.endsWith('.ts') || fileName.endsWith('.tsx')) {
      issues.push({
        type: 'bug',
        severity: 'low',
        line: Math.floor(Math.random() * 20) + 5,
        message: 'TypeScript best practices review recommended',
        code: '// Consider stricter type checking'
      })
    }
    
    // General code quality
    if (Math.random() > 0.5) { // 50% chance
      issues.push({
        type: 'smell',
        severity: 'low',
        line: Math.floor(Math.random() * 30) + 10,
        message: 'Code quality review - consider refactoring for better maintainability',
        code: '// General code quality review'
      })
    }
  }
  
  return issues
}
